{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Importes necesarios",
   "id": "e96698fa29069aaf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T20:21:50.624023Z",
     "start_time": "2026-01-24T20:21:50.478243900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import polars as pl\n",
    "import os\n",
    "import zipfile\n",
    "import orjson as json\n",
    "import tqdm as tqdm # Usamos .notebook para barras de carga bonitas en Jupyter\n",
    "\n",
    "# --- CONFIGURACI√ìN ---\n",
    "# Ajusta estas rutas a tus carpetas reales.\n",
    "# Si est√°s en Windows y usas backslashes (\\), usa r\"./data/KR\"\n",
    "CARPETA_DATA = r\"./Data/\"\n",
    "\n",
    "DATA_PATHS = {\n",
    "    \"EUW\": CARPETA_DATA + r\"matches_raw_euw_ranked.zip\",\n",
    "    \"KR\": CARPETA_DATA + r\"matches_raw_kr_ranked.zip\",\n",
    "    \"NA\": CARPETA_DATA + r\"matches_raw_na_ranked.zip\"\n",
    "}\n",
    "OUTPUT_FILE = \"draft_oracle_master_data.parquet\"\n",
    "\n",
    "# Verificaci√≥n r√°pida: ¬øExisten las carpetas?\n",
    "print(f\"Directorio de trabajo actual: {os.getcwd()}\")\n",
    "for region, path in DATA_PATHS.items():\n",
    "    exists = os.path.exists(path)\n",
    "    status = \"‚úÖ Encontrada\" if exists else \"‚ùå NO ENCONTRADA (Revisa la ruta)\"\n",
    "    print(f\"Regi√≥n {region} ({path}): {status}\")"
   ],
   "id": "a6b829279152a091",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio de trabajo actual: D:\\Proyectos\\SkyTheLimit\n",
      "Regi√≥n EUW (./Data/matches_raw_euw_ranked.zip): ‚úÖ Encontrada\n",
      "Regi√≥n KR (./Data/matches_raw_kr_ranked.zip): ‚úÖ Encontrada\n",
      "Regi√≥n NA (./Data/matches_raw_na_ranked.zip): ‚úÖ Encontrada\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Definici√≥n de la Funci√≥n de Extracci√≥n",
   "id": "da81a7613dd31722"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T20:21:50.647187900Z",
     "start_time": "2026-01-24T20:21:50.638652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_features_complete(data, region, filename=\"unknown\"):\n",
    "    \"\"\"\n",
    "    Combina las m√©tricas Base (v1) y Pro (v2) en un solo registro.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        info = data.get('info', {})\n",
    "        meta = data.get('metadata', {})\n",
    "\n",
    "        # Filtros\n",
    "        if info.get('queueId', 0) != 420 or info.get('gameDuration', 0) < 900:\n",
    "            return []\n",
    "\n",
    "        match_id = meta.get('matchId', filename)\n",
    "\n",
    "        extracted_rows = []\n",
    "        for p in info.get('participants', []):\n",
    "            challenges = p.get('challenges', {})\n",
    "            perks = p.get('perks', {})\n",
    "\n",
    "            # C√°lculo DCR\n",
    "            dmg = p.get('totalDamageDealtToChampions', 0)\n",
    "            gold = p.get('goldEarned', 1)\n",
    "            dcr = dmg / gold if gold > 0 else 0\n",
    "\n",
    "            # Runas\n",
    "            try:\n",
    "                styles = perks.get('styles', [])\n",
    "                primary = styles[0].get('style', -1) if styles else -1\n",
    "                keystone = styles[0].get('selections', [])[0].get('perk', -1) if styles and styles[0].get('selections') else -1\n",
    "            except:\n",
    "                primary = -1; keystone = -1\n",
    "\n",
    "            row = {\n",
    "                # --- IDENTIFICADORES ---\n",
    "                \"game_id\": match_id, \"region\": region, \"patch\": info.get('gameVersion', \"0.0.0\"),\n",
    "                \"champ_id\": p.get('championId'), \"position\": p.get('teamPosition', 'UTILITY'),\n",
    "                \"side\": p.get('teamId'), \"target\": 1 if p.get('win') else 0, \"duration\": info.get('gameDuration', 0), \"queue\": info.get('queueId', 0),\n",
    "\n",
    "                # --- 1. PERFIL DE DA√ëO (BASE) ---\n",
    "                \"stat_phys_dmg\": p.get('physicalDamageDealtToChampions', 0),\n",
    "                \"stat_magic_dmg\": p.get('magicDamageDealtToChampions', 0),\n",
    "                \"stat_true_dmg\": p.get('trueDamageDealtToChampions', 0),\n",
    "                \"stat_dmg_taken\": p.get('totalDamageTaken', 0),\n",
    "                \"stat_mitigated\": p.get('damageSelfMitigated', 0),\n",
    "                \"stat_heal\": p.get('totalHealsOnTeammates', 0),\n",
    "\n",
    "                # --- 2. PERFIL DE UTILIDAD (BASE + PRO) ---\n",
    "                \"stat_cc_duration\": p.get('timeCCingOthers', 0),\n",
    "                \"stat_hard_cc\": challenges.get('enemyChampionImmobilizations', 0),\n",
    "                \"stat_vision\": challenges.get('visionScorePerMinute', 0),\n",
    "\n",
    "                # --- 3. ECONOM√çA Y MACRO (PRO) ---\n",
    "                \"stat_dcr\": dcr, # Nuevo\n",
    "                \"stat_dpm\": challenges.get('damagePerMinute', 0),\n",
    "                \"stat_gpm\": challenges.get('goldPerMinute', 0),\n",
    "                \"stat_turret_plates\": challenges.get('turretPlatesTaken', 0),\n",
    "                \"stat_obj_control\": challenges.get('dragonTakedowns', 0),\n",
    "\n",
    "                # --- 4. LANE & ROAMING (PRO) ---\n",
    "                \"stat_lane_cs_10\": challenges.get('laneMinionsFirst10Minutes', 0), # Faltaba este!\n",
    "                \"stat_solo_kills\": challenges.get('soloKills', 0),\n",
    "                \"stat_lane_diff\": challenges.get('laningPhaseGoldExpAdvantage', 0),\n",
    "                \"stat_roam_kills\": challenges.get('killsOnOtherLanesEarlyJungleAsLaner', 0),\n",
    "                \"stat_dodge\": challenges.get('skillshotsDodged', 0),\n",
    "\n",
    "                # --- T√ÅCTICAS ---\n",
    "                \"rune_primary\": primary, \"rune_keystone\": keystone\n",
    "            }\n",
    "            extracted_rows.append(row)\n",
    "        return extracted_rows\n",
    "    except: return []"
   ],
   "id": "3d2ec48e5a87c889",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "42628792e1e2b9e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T20:24:51.036181900Z",
     "start_time": "2026-01-24T20:21:50.647187900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- PAR√ÅMETROS DE RENDIMIENTO ---\n",
    "BATCH_SIZE = 5000  # Convertir a Polars cada 5000 partidas para liberar RAM\n",
    "chunks_list = []   # Lista de DataFrames optimizados\n",
    "current_batch = [] # Lista temporal de filas\n",
    "\n",
    "print(\"üöÄ Iniciando Ingesta (Soporte para Archivos ZIP Directos)...\")\n",
    "\n",
    "def flush_batch():\n",
    "    if not current_batch: return\n",
    "    try:\n",
    "        df_chunk = pl.DataFrame(current_batch)\n",
    "        # Casting Masivo de todas las columnas 'stat_' a Float32 para ahorrar RAM\n",
    "        # Detectamos din√°micamente las columnas que empiezan por 'stat_'\n",
    "        stat_cols = [c for c in df_chunk.columns if c.startswith(\"stat_\")]\n",
    "\n",
    "        # Schema de optimizaci√≥n\n",
    "        optimizations = [\n",
    "            pl.col(\"game_id\").cast(pl.Utf8),\n",
    "            pl.col(\"region\").cast(pl.Categorical),\n",
    "            pl.col(\"position\").cast(pl.Categorical),\n",
    "            pl.col(\"patch\").cast(pl.Utf8),\n",
    "            pl.col(\"target\").cast(pl.Int8),\n",
    "            pl.col(\"side\").cast(pl.Int16),\n",
    "            pl.col(\"champ_id\").cast(pl.Int16),\n",
    "            pl.col(\"rune_primary\").cast(pl.Int16),\n",
    "            pl.col(\"rune_keystone\").cast(pl.Int16),\n",
    "            pl.col(\"duration\").cast(pl.Int16),\n",
    "            pl.col(\"queue\").cast(pl.Int16)\n",
    "        ] + [pl.col(c).cast(pl.Float32) for c in stat_cols] # Todo lo stat_ a float\n",
    "\n",
    "        df_chunk = df_chunk.with_columns(optimizations)\n",
    "        chunks_list.append(df_chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error chunk: {e}\")\n",
    "    current_batch.clear()\n",
    "\n",
    "# --- BUCLE PRINCIPAL ---\n",
    "for region, path in DATA_PATHS.items():\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"‚ùå Ruta no encontrada: {path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"üìÇ Procesando Regi√≥n {region} en: {path}\")\n",
    "\n",
    "    # L√ìGICA H√çBRIDA: ¬øEs un archivo ZIP o una Carpeta?\n",
    "\n",
    "    # CASO A: Es un archivo ZIP directo (Tu configuraci√≥n actual)\n",
    "    if os.path.isfile(path) and path.endswith(\".zip\"):\n",
    "        try:\n",
    "            with zipfile.ZipFile(path, \"r\") as z:\n",
    "                all_json_names = [f for f in z.namelist() if f.endswith(\".json\")]\n",
    "\n",
    "                # Barra de carga para los archivos DENTRO del zip\n",
    "                for json_name in tqdm.tqdm(all_json_names, desc=f\"üì¶ Extrayendo {region}\", unit=\"json\"):\n",
    "                    with z.open(json_name) as f:\n",
    "                        try:\n",
    "                            # Leemos y parseamos\n",
    "                            content = f.read()\n",
    "                            data = json.loads(content)\n",
    "\n",
    "                            # Extraemos features\n",
    "                            rows = extract_features_complete(data, region, filename=json_name)\n",
    "                            current_batch.extend(rows)\n",
    "\n",
    "                            # Batch flush\n",
    "                            if len(current_batch) >= BATCH_SIZE * 10:\n",
    "                                flush_batch()\n",
    "                        except Exception:\n",
    "                            continue\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error cr√≠tico leyendo el ZIP {path}: {e}\")\n",
    "\n",
    "    # CASO B: Es una Carpeta (Configuraci√≥n antigua)\n",
    "    elif os.path.isdir(path):\n",
    "        files_in_folder = os.listdir(path)\n",
    "        for filename in tqdm.tqdm(files_in_folder, desc=f\"üìÇ Carpeta {region}\"):\n",
    "            full_path = os.path.join(path, filename)\n",
    "\n",
    "            # (Reutilizamos la l√≥gica de zip/json aqu√≠ si fuera necesario,\n",
    "            # pero tu error indica que est√°s en el Caso A)\n",
    "            if filename.endswith(\".json\"):\n",
    "                 try:\n",
    "                    with open(full_path, \"rb\") as f: # rb para compatibilidad con orjson\n",
    "                        data = json.loads(f.read())\n",
    "                        rows = extract_features_complete(data, region, filename=filename)\n",
    "                        current_batch.extend(rows)\n",
    "                        if len(current_batch) >= BATCH_SIZE * 10:\n",
    "                            flush_batch()\n",
    "                 except: continue\n",
    "\n",
    "# Procesar el √∫ltimo remanente\n",
    "flush_batch()\n",
    "\n",
    "print(f\"‚úÖ Ingesta finalizada. Se generaron {len(chunks_list)} chunks optimizados.\")"
   ],
   "id": "5073c841cd192c52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Iniciando Ingesta (Soporte para Archivos ZIP Directos)...\n",
      "üìÇ Procesando Regi√≥n EUW en: ./Data/matches_raw_euw_ranked.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üì¶ Extrayendo EUW: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 210238/210238 [01:50<00:00, 1906.59json/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Procesando Regi√≥n KR en: ./Data/matches_raw_kr_ranked.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üì¶ Extrayendo KR: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61381/61381 [00:32<00:00, 1886.76json/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Procesando Regi√≥n NA en: ./Data/matches_raw_na_ranked.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üì¶ Extrayendo NA: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 69393/69393 [00:36<00:00, 1922.16json/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ingesta finalizada. Se generaron 59 chunks optimizados.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "543816533c536af6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T20:24:51.827925500Z",
     "start_time": "2026-01-24T20:24:51.114834600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if len(chunks_list) > 0:\n",
    "    print(\"üß© Unificando chunks en un solo DataFrame Maestro...\")\n",
    "\n",
    "    # concat es muy eficiente porque los chunks ya tienen el mismo esquema y tipos\n",
    "    try:\n",
    "        df_master = pl.concat(chunks_list)\n",
    "\n",
    "        print(f\"üíæ Guardando {len(df_master)} registros en {OUTPUT_FILE}...\")\n",
    "        df_master.write_parquet(OUTPUT_FILE, compression=\"snappy\")\n",
    "\n",
    "        print(f\"üéâ ¬°ETL Completado! Dataset Final: {df_master.shape}\")\n",
    "\n",
    "        # Liberar memoria de la lista de chunks\n",
    "        del chunks_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al concatenar: {e}. Verifica que todos los chunks tengan las mismas columnas.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se extrajeron datos.\")"
   ],
   "id": "a3ab4cd87a341846",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß© Unificando chunks en un solo DataFrame Maestro...\n",
      "üíæ Guardando 2928910 registros en draft_oracle_master_data.parquet...\n",
      "üéâ ¬°ETL Completado! Dataset Final: (2928910, 30)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "dd98c5b6b15f5865"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T20:24:51.844537700Z",
     "start_time": "2026-01-24T20:24:51.837007300Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8b3df52ba57d5ecd",
   "outputs": [],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
